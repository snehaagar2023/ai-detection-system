{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# Load original HC3 dataset\n",
    "# ------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_hc3 = load_dataset(\"Hello-SimpleAI/HC3\", \"all\")\n",
    "rows = []\n",
    "for entry in ds_hc3[\"train\"]:\n",
    "    for a in entry[\"chatgpt_answers\"]:\n",
    "        rows.append({\"text\": a.strip(), \"label\": 0})  # AI\n",
    "    for a in entry[\"human_answers\"]:\n",
    "        rows.append({\"text\": a.strip(), \"label\": 1})  # Human\n",
    "df_hc3 = pd.DataFrame(rows).dropna()\n",
    "\n",
    "# Balance HC3 dataset\n",
    "min_class = df_hc3['label'].value_counts().min()\n",
    "data_balanced_hc3 = pd.concat([\n",
    "    df_hc3[df_hc3['label'] == 0].sample(min_class),\n",
    "    df_hc3[df_hc3['label'] == 1].sample(min_class)\n",
    "]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# Load IELTS dataset\n",
    "# ------------------------------------------------------\n",
    "ds_ielts = load_dataset(\"chillies/IELTS-writing-task-2-evaluation\")\n",
    "texts_ielts = [example['essay'] for example in ds_ielts['train']]\n",
    "labels_ielts = [1] * len(texts_ielts)\n",
    "\n",
    "df_ielts = pd.DataFrame({\n",
    "    \"text\": texts_ielts,\n",
    "    \"label\": labels_ielts\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after merging: 63639\n",
      "label\n",
      "1    36736\n",
      "0    26903\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Merge both datasets\n",
    "# ------------------------------------------------------\n",
    "# Combine HC3 (balanced) and IELTS (human essays)\n",
    "merged_df = pd.concat([data_balanced_hc3, df_ielts]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Total samples after merging:\", len(merged_df))\n",
    "print(merged_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Tokenization & Dataset Prep\n",
    "# ------------------------------------------------------\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = CombinedDataset(merged_df['text'].tolist(), merged_df['label'].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Load pretrained trained model\n",
    "# ------------------------------------------------------\n",
    "from transformers import T5ForSequenceClassification\n",
    "\n",
    "model = T5ForSequenceClassification.from_pretrained(\"t5-small\", num_labels=2,)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sneha/venv/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 7955/7955 [13:00<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 479.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7955/7955 [13:07<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 85.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7955/7955 [13:06<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 44.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Fine-tuning\n",
    "# ------------------------------------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-ethics-ai-human-detector/tokenizer_config.json',\n",
       " 't5-ethics-ai-human-detector/special_tokens_map.json',\n",
       " 't5-ethics-ai-human-detector/spiece.model',\n",
       " 't5-ethics-ai-human-detector/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model weights and tokenizer after fine-tuning\n",
    "model.save_pretrained(\"t5-ethics-ai-human-detector\")\n",
    "tokenizer.save_pretrained(\"t5-ethics-ai-human-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported file: README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating reddit_chatGPT.jsonl:   0%|          | 0/750 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Evaluating reddit_chatGPT.jsonl: 100%|██████████| 750/750 [00:59<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for reddit_chatGPT.jsonl =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.95      0.88      0.91      3000\n",
      "Human-written       0.89      0.96      0.92      3000\n",
      "\n",
      "     accuracy                           0.92      6000\n",
      "    macro avg       0.92      0.92      0.92      6000\n",
      " weighted avg       0.92      0.92      0.92      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating toefl.json: 100%|██████████| 12/12 [00:00<00:00, 48.89it/s]\n",
      "/home/sneha/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sneha/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sneha/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for toefl.json =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.00      0.00      0.00         0\n",
      "Human-written       1.00      0.79      0.88        91\n",
      "\n",
      "     accuracy                           0.79        91\n",
      "    macro avg       0.50      0.40      0.44        91\n",
      " weighted avg       1.00      0.79      0.88        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating arxiv_chatGPT.jsonl: 100%|██████████| 750/750 [00:45<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for arxiv_chatGPT.jsonl =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.97      0.76      0.85      3000\n",
      "Human-written       0.80      0.97      0.88      3000\n",
      "\n",
      "     accuracy                           0.87      6000\n",
      "    macro avg       0.88      0.87      0.87      6000\n",
      " weighted avg       0.88      0.87      0.87      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating reddit_cohere.jsonl: 100%|██████████| 750/750 [00:44<00:00, 16.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for reddit_cohere.jsonl =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.96      0.98      0.97      3000\n",
      "Human-written       0.98      0.96      0.97      3000\n",
      "\n",
      "     accuracy                           0.97      6000\n",
      "    macro avg       0.97      0.97      0.97      6000\n",
      " weighted avg       0.97      0.97      0.97      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating arxiv_cohere.jsonl: 100%|██████████| 750/750 [00:50<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for arxiv_cohere.jsonl =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.97      0.75      0.84      3000\n",
      "Human-written       0.79      0.97      0.88      3000\n",
      "\n",
      "     accuracy                           0.86      6000\n",
      "    macro avg       0.88      0.86      0.86      6000\n",
      " weighted avg       0.88      0.86      0.86      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hewlett.json: 100%|██████████| 11/11 [00:00<00:00, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for hewlett.json =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Print classification report for each file\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI-generated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuman-written\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2567\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2561\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2563\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[1;32m   2564\u001b[0m             )\n\u001b[1;32m   2565\u001b[0m         )\n\u001b[1;32m   2566\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2573\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import MT5Tokenizer, MT5ForSequenceClassification\n",
    "\n",
    "# Load classification model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForSequenceClassification.from_pretrained(\"t5-ethics-ai-human-detector\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "dev_dir = \"cs162-final-dev-main\"\n",
    "\n",
    "# Loop over all files in your folder\n",
    "for filename in os.listdir(dev_dir):\n",
    "    file_path = os.path.join(dev_dir, filename)\n",
    "    texts, true_labels = [], []\n",
    "\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        # jsonlines format (human_text / machine_text)\n",
    "        human_texts, machine_texts = [], []\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for row in reader:\n",
    "                human_texts.append(row[\"human_text\"])\n",
    "                machine_texts.append(row[\"machine_text\"])\n",
    "        texts = human_texts + machine_texts\n",
    "        true_labels = [1] * len(human_texts) + [0] * len(machine_texts)\n",
    "\n",
    "    elif filename.endswith(\".json\"):\n",
    "        # plain json (hewlett.json, toefl.json)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            texts = [entry[\"document\"] for entry in data]\n",
    "            true_labels = [1] * len(texts)\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping unsupported file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Inference loop for classification\n",
    "    preds = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), 8), desc=f\"Evaluating {filename}\"):\n",
    "        batch_texts = texts[i:i+8]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            batch_preds = torch.argmax(logits, dim=1).tolist()\n",
    "            preds.extend(batch_preds)\n",
    "\n",
    "    # Print classification report for each file\n",
    "    print(f\"\\n===== Results for {filename} =====\")\n",
    "    print(classification_report(true_labels, preds, target_names=[\"AI-generated\", \"Human-written\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n",
      "Evaluating hewlett.json:   0%|          | 0/11 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Evaluating hewlett.json: 100%|██████████| 11/11 [00:00<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for hewlett.json =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.00      0.00      0.00         0\n",
      "Human-written       1.00      1.00      1.00        88\n",
      "\n",
      "    micro avg       1.00      1.00      1.00        88\n",
      "    macro avg       0.50      0.50      0.50        88\n",
      " weighted avg       1.00      1.00      1.00        88\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating german_wikipedia.jsonl: 100%|██████████| 125/125 [00:10<00:00, 11.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results for german_wikipedia.jsonl =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " AI-generated       0.20      0.01      0.01       500\n",
      "Human-written       0.50      0.98      0.66       500\n",
      "\n",
      "     accuracy                           0.49      1000\n",
      "    macro avg       0.35      0.49      0.33      1000\n",
      " weighted avg       0.35      0.49      0.33      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import MT5Tokenizer, MT5ForSequenceClassification\n",
    "\n",
    "# Load your classification model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MT5ForSequenceClassification.from_pretrained(\"t5-ethics-ai-human-detector\").to(device)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Directory where eval files live\n",
    "dev_dir = \"cs162-final-dev-main\"\n",
    "\n",
    "# Files to evaluate\n",
    "files_to_evaluate = [\"hewlett.json\", \"german_wikipedia.jsonl\"]\n",
    "\n",
    "# Loop over the selected files\n",
    "for filename in files_to_evaluate:\n",
    "    file_path = os.path.join(dev_dir, filename)\n",
    "    texts, true_labels = [], []\n",
    "\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        # jsonlines format (human_text / machine_text)\n",
    "        human_texts, machine_texts = [], []\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for row in reader:\n",
    "                human_texts.append(row[\"human_text\"])\n",
    "                machine_texts.append(row[\"machine_text\"])\n",
    "        texts = human_texts + machine_texts\n",
    "        true_labels = [1] * len(human_texts) + [0] * len(machine_texts)\n",
    "\n",
    "    elif filename.endswith(\".json\"):\n",
    "        # plain json (hewlett.json)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            texts = [entry[\"document\"] for entry in data]\n",
    "            true_labels = [1] * len(texts)  # all human-written\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping unsupported file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Inference loop for classification\n",
    "    preds = []\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 8\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Evaluating {filename}\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            batch_preds = torch.argmax(logits, dim=1).tolist()\n",
    "            preds.extend(batch_preds)\n",
    "\n",
    "    # Print classification report for each file\n",
    "    print(f\"\\n===== Results for {filename} =====\")\n",
    "    print(classification_report(\n",
    "        true_labels,\n",
    "        preds,\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"AI-generated\", \"Human-written\"],\n",
    "        zero_division=0\n",
    "    ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
